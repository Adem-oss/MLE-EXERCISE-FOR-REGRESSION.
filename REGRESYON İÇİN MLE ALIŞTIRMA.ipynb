{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0ed686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4a5fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent(x,y,max_iter=1000, lr = 0.01, normalize = False):\n",
    "    if normalize:\n",
    "        x = x/y\n",
    "        y = y/y\n",
    "    tolerance = 10**(-3)\n",
    "    grad = 92\n",
    "    a = np.random.random()\n",
    "    iter_ = 0\n",
    "    print(\"a'nın başlangıç değeri: \",a)\n",
    "    while (abs(grad) > tolerance) and (max_iter > iter_):\n",
    "        grad = 2 * (y - a*x)*(-x)\n",
    "        a = a - lr * grad\n",
    "        if iter_ % 100 == 0:\n",
    "            print(f\"gradient iterasyonu {iter_}, gradient {grad}\")\n",
    "        iter_ += 1\n",
    "    print(\"son iterasyon no:\", iter_)\n",
    "    print(\"mevcut gradient:\", grad)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c88a45ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a'nın başlangıç değeri:  0.9169088916975947\n",
      "gradient iterasyonu 0, gradient -0.6647288664192423\n",
      "son iterasyon no: 79\n",
      "mevcut gradient: -0.0009956233461858588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9998855033151887"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientdescent(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b04e7410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.1 + 0.1 == 0.3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6d7ae0f",
   "metadata": {},
   "source": [
    "Bu kod bir gradient descent optimizasyon algoritmasıdır.\n",
    "\n",
    "Gradient descent, bir fonksiyonun minimum noktasına doğru ilerleyen bir optimizasyon algoritmasıdır. Bu algoritmanın genel mantığı, bir başlangıç noktasından başlayarak, her adımda fonksiyonun gradientini (eğimini) hesaplayarak, gradientin ters yöne hareket ederek fonksiyonun minimum noktasına doğru ilerlemektir. Bu işlem, aynı zamanda birçok makine öğrenmesi algoritması için kullanılan bir optimizasyon yöntemidir.\n",
    "\n",
    "Bu özel kod, verilen x ve y dizileri için gradient descent yöntemini kullanarak doğrusal bir regresyon modeli için optimum eğim değerini hesaplar. Varsayılan olarak, maksimum iterasyon sayısı 1000 olarak ayarlanmıştır ve öğrenme oranı 0.01 olarak ayarlanmıştır. İsteğe bağlı olarak, veriler normalize edilebilir.\n",
    "\n",
    "kodun ne yaptığına adım adım bakalım:\n",
    "\n",
    "Fonksiyonun girdileri arasında \"normalize\" adında bir parametre var. Bu parametre varsayılan olarak False'dur, yani normalize edilmeden hesaplama yapılacak demektir. Eğer bu parametre True olarak belirtilirse, x ve y değişkenleri önce normalize edilir.\n",
    "Tolerance değişkeni tanımlanır ve değeri 10^-3 olarak atanır. Bu, algoritmanın duracağı tolerans seviyesini belirler.\n",
    "Grad değişkeni tanımlanır ve değeri 92 olarak atanır. Bu sadece bir başlangıç değeri olarak kullanılacaktır.\n",
    "a değişkeni np.random.random() fonksiyonu kullanılarak rastgele bir sayı atanır. Bu, a'nın başlangıç değerini belirler.\n",
    "iter_ değişkeni tanımlanır ve değeri sıfıra eşitlenir. Bu değişken, kaç tane iterasyon yapıldığını takip etmek için kullanılacak.\n",
    "\"a'nın başlangıç değeri\" mesajı yazdırılır. Bu, kullanıcının hangi başlangıç değerinin kullanıldığına dair bilgi verir.\n",
    "Bir while döngüsü tanımlanır. Bu döngü grad değişkeni tolerance değerinden büyük olduğu sürece çalışacaktır. Aynı zamanda, iterasyon sayısı max_iter değerinden küçük olduğu sürece de çalışacaktır. Bu, sonsuz bir döngüye girme ihtimalini önler.\n",
    "grad değişkeni hesaplanır. Bu, a ve x değerlerine bağlı olan bir değerdir.\n",
    "a değişkeni grad ve learning rate (lr) kullanılarak güncellenir.\n",
    "Eğer iterasyon sayısı 100'e tam bölünüyorsa, \"gradient iterasyonu\" ve \"gradient\" mesajları yazdırılır. Bu, kullanıcının algoritmanın ne kadar sürede tamamlandığı hakkında bilgi sahibi olmasını sağlar.\n",
    "iter_ değişkeni bir arttırılır.\n",
    "Döngü sona erdiğinde, \"son iterasyon no\" ve \"mevcut gradient\" mesajları yazdırılır. Bu, algoritmanın son iterasyonunda hangi değerlerin kullanıldığını belirtir.\n",
    "Son olarak, a değişkeni döndürülür. Bu, hesaplanan doğruyun eğimini temsil eder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
