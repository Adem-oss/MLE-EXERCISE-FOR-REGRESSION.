{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0ed686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a5fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent(x,y,max_iter=1000, lr = 0.01, normalize = False):\n",
    "    if normalize:\n",
    "        x = x/y\n",
    "        y = y/y\n",
    "    tolerance = 10**(-3)\n",
    "    grad = 92\n",
    "    a = np.random.random()\n",
    "    iter_ = 0\n",
    "    print(\"a initialize value: \",a)\n",
    "    while (abs(grad) > tolerance) and (max_iter > iter_):\n",
    "        grad = 2 * (y - a*x)*(-x)\n",
    "        a = a - lr * grad\n",
    "        if iter_ % 100 == 0:\n",
    "            print(f\"gradient iteration {iter_}, gradient {grad}\")\n",
    "        iter_ += 1\n",
    "    print(\"last iteration no:\", iter_)\n",
    "    print(\"current gradient:\", grad)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88a45ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a initialize value:  0.05370913237369268\n",
      "gradient iteration 0, gradient -7.5703269410104586\n",
      "gradient iteration 100, gradient -0.0018109120993257122\n",
      "last iteration no: 109\n",
      "current gradient: -0.0009293942669685862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9998931196592986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientdescent(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04e7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 + 0.1 + 0.1 == 0.3   it is interesting that, due to the python algoritm it returns False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7ae0f",
   "metadata": {},
   "source": [
    "This code is a gradient descent optimization algorithm.\n",
    "\n",
    "Gradient descent is an optimization algorithm that moves towards the minimum point of a function. The general logic of this algorithm is to start from a starting point, calculate the gradient (slope) of the function at each step, and move towards the minimum point of the function by moving in the opposite direction of the gradient. This process is also an optimization method used for many machine learning algorithms.\n",
    "\n",
    "This particular code calculates the optimum slope value for a linear regression model using the gradient descent method for the given x and y sequences. By default, the maximum number of iterations is set to 1000 and the learning rate is set to 0.01. Optionally, the data can be normalized.\n",
    "\n",
    "Let's take a step-by-step look at what the code does:\n",
    "\n",
    "Among the inputs of the function is a parameter called \"normalize\". This parameter is False by default, meaning the calculation will be performed without normalizing. If this parameter is specified as True, the x and y variables are normalized first.\n",
    "The variable tolerance is defined and its value is set to 10^-3. This determines the tolerance level at which the algorithm will stop.\n",
    "The grad variable is defined and its value is set to 92. This will only be used as an initial value.\n",
    "The variable a is assigned a random number using the np.random.random() function. This sets the initial value of a.\n",
    "The variable iter_ is defined and its value is set to zero. This variable will be used to keep track of how many iterations have been made.\n",
    "The message \"initial value of a\" is printed. This gives the user information on which initial value is used.\n",
    "A while loop is defined. This loop will run as long as the grad variable is greater than tolerance. It will also work as long as the number of iterations is less than max_iter. This avoids the possibility of getting into an infinite loop.\n",
    "grad variable is calculated. This is a value that depends on the values of a and x.\n",
    "The variable a is updated using grad and learning rate (lr).\n",
    "If the number of iterations is divisible by 100, the \"gradient iteration\" and \"gradient\" messages are printed. This allows the user to know how long it took the algorithm to complete.\n",
    "The iter_ variable is incremented by one.\n",
    "When the loop ends, the messages \"last iteration number\" and \"current gradient\" are printed. This specifies which values are used in the last iteration of the algorithm.\n",
    "Finally, the variable a is returned. This represents the slope of the calculated line."
   ]
  },
  {
   "cell_type": "raw",
   "id": "90f84257",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
